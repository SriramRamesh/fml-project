\section{Conclusion}

While optimizing clustering still has a long way to go in reaching the state of the art in adversarial robustness, its inherent link to avoiding adversarial examples makes it very appealing from a theoretical standpoint. Prior work showed that this does give positive results for certain restricted adversaries; we showed that we can achieve some robustness even for more sophisticated ones. Clustering based classifiers inherently carry meaningful distance metrics that can be used to understand adversarial robustness better. Hence, the problem of optimizing adversarially robust clustering shouldn't only be studied for the sake of improving state of art in adversarial robustness, but also for getting deeper insights into the problem of robustness. With this train of thought, we mention the following open problems for further study.


\subsection{Further Directions}

This work represents an early investigation into the domain of clustering as a robust classification technique, and we are confident that further improvements can be made. While we began the search into an optimal number of clusters, our results are by no means complete, theoretically or experimentally. Other clustering techniques, such as fuzzy $c$-means~\cite{Bezdek1984FCM}, may also have better results. While we used the Magnet Loss, optimizing for other losses may also result in better-clustered data. These results may also depend on the type of classification problem.

Because the robust clustering functions as a fortification for existing techniques, more work is needed to understand if there are certain robustness techniques that "play nice" with clustering. We propose looking into better methods of transforming embeddings into logits as a possible direction for this. While TRADES was used because it is a powerful existing classifier, other algorithms may give better or more efficient experimental results. It is also possible a better loss may exist tailor-made to achieve clustering robustness. We also propose looking into L2 robustness of clustering based classifiers as a worthwhile direction because clustering methods inherently have some L2 robustness guarantees. Replacing warm start by adversarial pretraining, might also give us surprising results.