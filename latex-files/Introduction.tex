\section{Introduction}

Deep neural networks (DNNs) have proven themselves to be very useful for classification problems, from facial recognition~\cite{Sun2015FaceRecog} to disease detection~\cite{Boulent2019PlantDisease}, natural language processing~\cite{Lai2015NLP}, and content moderation~\cite{Pavlopoulos2017Moderation}. One desirable property of deep neural networks is \emph{adversarial robustness}~\cite{Szegedy2013Robust}, or resistance to misclassifying slightly tampered inputs. Due to the stakes inherent in many applications as well as the desire to subvert classification in many settings, strong robustness is needed even in the worst-case setting, where a potentially powerful adversary tries to misclassify inputs.

Perhaps unsurprisingly, not all DNNs are equally adept at adversarial robustness. In this work, we focus on one technique in classification: Clustering. Abstractly, clustering can be used to great effect in machine learning by allowing a classifier to separate the training data into more distinct classification groups. Then, testing reduces to finding the correct cluster to associate to the input point. Clustering classifiers may also be thought of as learning a distance metric~\cite{Weinberger2009Distance}, which can then be used to measure the output class of inputs. Specifically, in this work, we focus on the technique of robust clustering losses~\cite{Alfarra2020ClusTR}, of which perhaps the most well-known example is the Magnet Loss, introduced by Rippel et al.~\cite{Rippel2015Magnet}


Alfarra et al.~\cite{Alfarra2020ClusTR} showed that minimizing the Magnet Loss results in parameters that magnify the difference between the classifier's distance to a "correct" (read: of the point's true class) centroid and the classifier's distance to any "incorrect" (of any other class) centroid. In turn, this creates a natural robustness radius around each centroid, ensuring that each point within this radius will be drawn into the same classification. To this end, they propose ClusTR, which first runs a DNN with typical Cross Entropy loss training before augmenting that training with some additional training using the Magnet Loss. In their findings, they see this results in strong robustness against projected gradient descent attacks (see~\cite[Table 1]{Alfarra2020ClusTR}), as introduced by Madry et al.~\cite{Madry2017Robust}

Unfortunately, ClusTR does not seem to fare as well against more sophisticated attackers. RobustBench~\cite{Croce2020RobustBench}, which uses AutoAttack~\cite{Croce2020AutoAttack} to find adversarial examples, claims ClusTR has no robustness against AutoAttack, achieving zero percent accuracy on the adversarial points generated. Perhaps even stranger, this result is much worse than TRADES~\cite{Zhang2019TRADES}, which ClusTR's experimental results augment and which manages to achieve 53.08 percent robustness accuracy on RobustBench. This result is surprising both experimentally — as AutoAttack seems to supercede claims of robustness in~\cite{Alfarra2020ClusTR} — and theoretically — as AutoAttack seems to find examples that subvert the robustness radius inherent to clustering algorithms. This raises a natural question that will be the focus of this work:
\begin{center}
    \emph{Can we achieve stronger adversarial robustness using clustering-based learning algorithms?}
\end{center}

\subsection{Our Contribution}

We give an overview of previous techniques, an investigation as to where they may fail, and some future directions to consider. Specifically, we note the following:
\begin{enumerate}
    \item The work of Alfarra et al.~\cite{Alfarra2020ClusTR} provides a good framework as to how clustering provides natural adversarial robustness, in particular observing that adversarial robustness and clustering both involve some notion of grouping semantically similar data points to the "same" class (See \cite[Figure 1]{Alfarra2020ClusTR}). We investigate these results and posit other directions to improve clustering robustness classifiers.
    \item The strategy of ClusTR put forth by Alfarra et al. ultimately fails against the more sophisticated attacks within RobustBench because of underdeveloped training. Specifically, their amendment to TRADES~\cite{Zhang2019TRADES} actually strips the protocol of its robustness, resulting in a network with false robustness against sophisticated adversaries. Note that this doesn't necessarily contradict the promises of their theoretical work as the underdeveloped training results in poor results in regards to all terms in their bound.
    \item Augmenting ClusTR by using more standard adversarial training results in a more adversarially robust network against the ensemble of attacks in AutoAttack. This shows there is still hope for clustering as a technique for robustness, despite the negative results on RobustBench. 
    \item We investigate possible directions that may further aid in studying clustering classifiers. We note in particular that relying on logit-based schemes (including TRADES) does not and should not result in optimal clustering classifiers, which instead rely on the learning of a distance metric. We conclude by giving a call-to-action on future directions.
\end{enumerate}