An adversarially robust classifier should inherently force similar inputs to the same output. Naturally, this is the same paradigm guiding classical clustering algorithms, such as $K$-nearest neighbors. It is surprising, then, that existing literature in the field trying to link these two concepts entirely fails at strong guarantees of adversarial robustness. Namely, ClusTR, introduced by Alfarra et al. in 2020, achieves no adversarial robustness against the ensemble of attacks on RobustBench.

In this work, we investigate why this may be the case. We give a background on existing methods of both measuring adversarial robustness and learning concepts through clustering. We investigate why ClusTR fails at adversarial robustness, despite their strong claims. We then present new theoretical bounds on clustering techniques, guiding design principles on what should increase robustness. Finally, we begin an experimental study as to how we can improve adversarial robustness. In this study, we find a method that does achieve nonzero adversarial robustness against AutoAttack, though we admit that this requires further testing in order to achieve a usable (in terms of robustness and accuracy guarantees) neural network. 

This work can be found at~\url{https://github.com/SriramRamesh/fml-project}.